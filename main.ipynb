{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU4bPOXNf594",
        "outputId": "c5bf54a0-a19a-48f9-af92-76558e20dc33"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>119105</th>\n",
              "      <td>Geez, are you forgetful!  We've already discus...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131631</th>\n",
              "      <td>Carioca RFA \\n\\nThanks for your support on my ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125326</th>\n",
              "      <td>\"\\n\\n Birthday \\n\\nNo worries, It's what I do ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111256</th>\n",
              "      <td>Pseudoscience category? \\n\\nI'm assuming that ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83590</th>\n",
              "      <td>(and if such phrase exists, it would be provid...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment_text  toxic\n",
              "119105  Geez, are you forgetful!  We've already discus...      0\n",
              "131631  Carioca RFA \\n\\nThanks for your support on my ...      0\n",
              "125326  \"\\n\\n Birthday \\n\\nNo worries, It's what I do ...      0\n",
              "111256  Pseudoscience category? \\n\\nI'm assuming that ...      0\n",
              "83590   (and if such phrase exists, it would be provid...      0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %%\n",
        "# Import the pandas library, commonly used for data manipulation and analysis, aliased as 'pd'.\n",
        "import pandas as pd\n",
        "# Import TensorFlow, the core open-source library for machine learning and deep learning.\n",
        "import tensorflow as tf\n",
        "# From TensorFlow's Keras API, import the Tokenizer class for converting text into numerical sequences.\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# Import the pad_sequences function to ensure all text sequences have the same length.\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Import the Sequential model type, which allows us to build a neural network layer by layer.\n",
        "from tensorflow.keras.models import Sequential\n",
        "# Import the specific types of layers we'll use in our neural network.\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "# Import a utility from scikit-learn to easily split our data into training and testing sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Import the pickle library to save our tokenizer object for later use in the web app.\n",
        "import pickle\n",
        "\n",
        "# [cite_start]Use pandas to read the dataset from a CSV file into a DataFrame called 'df'. [cite: 18]\n",
        "df = pd.read_csv('data/train.csv')\n",
        "\n",
        "# Define a list containing the names of all columns that indicate different types of toxicity.\n",
        "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "# Create a new, single 'toxic' column. If a comment is flagged in ANY of the label_cols, this new column will be 1 (True), otherwise 0 (False).\n",
        "# .any(axis=1) checks for any True value across the columns for each row.\n",
        "# .astype(int) converts the resulting boolean (True/False) to an integer (1/0).\n",
        "df['toxic'] = df[label_cols].any(axis=1).astype(int)\n",
        "\n",
        "# To make training faster for this example, we'll work with a smaller, random subset of the data.\n",
        "# First, we select only the columns we need: 'comment_text' (our feature) and 'toxic' (our label).\n",
        "# Then, we .sample() 50,000 rows randomly.\n",
        "# random_state=42 ensures that we get the exact same \"random\" sample every time we run this code, making our results reproducible.\n",
        "df = df[['comment_text', 'toxic']].sample(50000, random_state=42)\n",
        "\n",
        "# Display the first 5 rows of our processed DataFrame to verify the changes.\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# Extract the 'comment_text' column from the DataFrame and convert it into a NumPy array. This will serve as our features (X).\n",
        "comments = df['comment_text'].values\n",
        "# Extract the 'toxic' column and convert it into a NumPy array. This will be our target labels (y).\n",
        "labels = df['toxic'].values\n",
        "\n",
        "# Define the maximum number of unique words to include in our vocabulary. We'll use the 10,000 most frequent words.\n",
        "vocab_size = 10000\n",
        "# Create an instance of the Tokenizer.\n",
        "# num_words specifies the vocabulary size.\n",
        "# oov_token='<OOV>' creates a special token for any words that are not in the vocabulary (Out-Of-Vocabulary words).\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "# \"Fits\" the tokenizer on our text data. This process builds the word index, mapping each unique word to an integer.\n",
        "tokenizer.fit_on_texts(comments)\n",
        "\n",
        "# Convert each comment text into a sequence of integers based on the word index created by the tokenizer.\n",
        "sequences = tokenizer.texts_to_sequences(comments)\n",
        "\n",
        "# Define the maximum length for each sequence. Comments longer than this will be cut, and shorter ones will be padded.\n",
        "max_length = 200\n",
        "# Apply padding to the sequences to ensure they all have the same length (max_length).\n",
        "# 'padding=\"post\"' adds zeros at the end of shorter sequences.\n",
        "# 'truncating=\"post\"' removes words from the end of longer sequences.\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (40000, 200)\n",
            "Testing data shape: (10000, 200)\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\sbhav\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# %%\n",
        "# Define the dimensionality of the word embedding vectors. Each word will be represented by a 16-dimensional vector.\n",
        "embedding_dim = 16\n",
        "\n",
        "# Create a Sequential model, which is a linear stack of layers. [cite_start]We will add layers one by one. [cite: 21]\n",
        "model = Sequential([\n",
        "    # The first layer is an Embedding layer. It turns the integer-encoded vocabulary into dense vectors of a fixed size (embedding_dim).\n",
        "    # vocab_size: The total number of words in our vocabulary.\n",
        "    # embedding_dim: The size of the vector for each word.\n",
        "    # input_length: The length of input sequences (200 in our case).\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    \n",
        "    # [cite_start]A Bidirectional LSTM layer. Bidirectional means the input is processed in both forward and backward directions, capturing context from both past and future words. [cite: 22]\n",
        "    # LSTM(64, ...): An LSTM layer with 64 internal units (neurons).\n",
        "    # return_sequences=True: This is important because the next layer is also an LSTM, which requires a sequence as input. This argument makes the layer output the full sequence of hidden states.\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    \n",
        "    # Another Bidirectional LSTM layer, this time with 32 units.\n",
        "    # By default, return_sequences is False, so this layer will only output the final hidden state, which summarizes the entire sequence's meaning.\n",
        "    Bidirectional(LSTM(32)),\n",
        "    \n",
        "    # A standard fully-connected (Dense) layer with 16 neurons.\n",
        "    # activation='relu': Uses the Rectified Linear Unit activation function to introduce non-linearity, allowing the model to learn more complex patterns.\n",
        "    Dense(16, activation='relu'),\n",
        "    \n",
        "    # The final output layer. It has a single neuron because this is a binary classification problem (toxic or not toxic).\n",
        "    # activation='sigmoid': The sigmoid activation function squashes the output to a value between 0 and 1, which can be interpreted as the probability of the comment being toxic.\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Configure the model for training.\n",
        "model.compile(loss='binary_crossentropy', # Specifies the loss function. 'binary_crossentropy' is the standard for two-class classification problems.\n",
        "              optimizer='adam',             # Specifies the optimization algorithm. 'adam' is a popular and effective choice.\n",
        "              metrics=['accuracy'])         # Specifies the metric to monitor during training. Here, we want to see the classification accuracy.\n",
        "\n",
        "# Print a summary of the model's architecture, including each layer's type, output shape, and number of parameters.\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 144ms/step - accuracy: 0.9334 - loss: 0.1978 - val_accuracy: 0.9588 - val_loss: 0.1320\n",
            "Epoch 2/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 88ms/step - accuracy: 0.9589 - loss: 0.1186 - val_accuracy: 0.9565 - val_loss: 0.1298\n",
            "Epoch 3/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 92ms/step - accuracy: 0.9679 - loss: 0.0903 - val_accuracy: 0.9583 - val_loss: 0.1219\n",
            "Epoch 4/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 102ms/step - accuracy: 0.9735 - loss: 0.0764 - val_accuracy: 0.9596 - val_loss: 0.1278\n",
            "Epoch 5/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 178ms/step - accuracy: 0.9768 - loss: 0.0664 - val_accuracy: 0.9606 - val_loss: 0.1336\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 33ms/step - accuracy: 0.9606 - loss: 0.1336\n",
            "Test Accuracy: 96.06%\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# %%\n",
        "# Save the trained model\n",
        "model.save('toxicity_model.h5')\n",
        "\n",
        "# Save the tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "print(\"Model and tokenizer saved successfully!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
